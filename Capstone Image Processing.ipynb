{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Using Keras and Tensorflow - Chris Kim\n",
    "\n",
    "This notebook was adapted from github user [nirmalyaghosh](https://github.com/nirmalyaghosh/deep-learning-vm), and user [fchollet](https://github.com/fchollet/keras/issues/3109). Lastly, most information was adapted from a great website called [Machine Learning Mastery](http://machinelearningmastery.com/).\n",
    "\n",
    "The code below is my first attempt to use image processing using the tools made available in Python, namely `keras` and `TensorFlow` as the backend. This exploration was inspired by [doyleax](https://doyleax.github.io/Portfolio/capstone.html) and her Capstone project. Side profile images of 1780 pairs of shoes are transformed to be evaluated in a neural network to try to predict whether a shoe is classified as rare or not (determined by a metric composed of retail price and average deadstock price).\n",
    "\n",
    "All images are taken from [StockX](www.stockx.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "np.random.seed(7)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1877, 200, 280, 3)\n",
      "(1877,)\n"
     ]
    }
   ],
   "source": [
    "with open('../../../keras/deep-learning-vm/notebooks/shoenumbers.pickle', 'rb') as handle:\n",
    "    X = pickle.load(handle, encoding='bytes')\n",
    "with open('../../../keras/deep-learning-vm/notebooks/targetvars.pickle', 'rb') as handle:\n",
    "    y = pickle.load(handle, encoding='bytes')\n",
    "\n",
    "print(np.array(X).shape)\n",
    "print(np.array(y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.503462972829\n"
     ]
    }
   ],
   "source": [
    "baseline = y.mean()\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1877, 100, 140, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change size of images so it's not so huge when we pass it through keras\n",
    "import cv2\n",
    "new_X = []\n",
    "for row in range(len(X)):\n",
    "    new_X.append(cv2.resize(X[row], (140,100)))\n",
    "np.array(new_X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for training and testing\n",
    "X = np.array(new_X)\n",
    "X = X.astype('float32')\n",
    "X /= 255.0\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation by machinelearningmastery\n",
    "num_classes=2\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(100,140,3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "epochs = 50\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1407 samples, validate on 470 samples\n",
      "Epoch 1/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.7484 - acc: 0.5146 - val_loss: 0.6968 - val_acc: 0.4468\n",
      "Epoch 2/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.6926 - acc: 0.5281 - val_loss: 0.7031 - val_acc: 0.4468\n",
      "Epoch 3/50\n",
      "1407/1407 [==============================] - 143s - loss: 0.6908 - acc: 0.5366 - val_loss: 0.6983 - val_acc: 0.4532\n",
      "Epoch 4/50\n",
      "1407/1407 [==============================] - 142s - loss: 0.6830 - acc: 0.5665 - val_loss: 0.7036 - val_acc: 0.4872\n",
      "Epoch 5/50\n",
      "1407/1407 [==============================] - 143s - loss: 0.6721 - acc: 0.5842 - val_loss: 0.6942 - val_acc: 0.4979\n",
      "Epoch 6/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.6434 - acc: 0.6404 - val_loss: 0.6728 - val_acc: 0.5723\n",
      "Epoch 7/50\n",
      "1407/1407 [==============================] - 143s - loss: 0.6194 - acc: 0.6588 - val_loss: 0.6846 - val_acc: 0.5936\n",
      "Epoch 8/50\n",
      "1407/1407 [==============================] - 142s - loss: 0.5976 - acc: 0.6802 - val_loss: 0.6730 - val_acc: 0.5447\n",
      "Epoch 9/50\n",
      "1407/1407 [==============================] - 143s - loss: 0.5712 - acc: 0.7050 - val_loss: 0.6552 - val_acc: 0.6213\n",
      "Epoch 10/50\n",
      "1407/1407 [==============================] - 142s - loss: 0.5376 - acc: 0.7306 - val_loss: 0.6682 - val_acc: 0.6128\n",
      "Epoch 11/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.5204 - acc: 0.7463 - val_loss: 0.6451 - val_acc: 0.6489\n",
      "Epoch 12/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.4683 - acc: 0.7697 - val_loss: 0.6562 - val_acc: 0.6723\n",
      "Epoch 13/50\n",
      "1407/1407 [==============================] - 142s - loss: 0.4477 - acc: 0.7889 - val_loss: 0.6424 - val_acc: 0.6596\n",
      "Epoch 14/50\n",
      "1407/1407 [==============================] - 142s - loss: 0.4319 - acc: 0.8053 - val_loss: 0.6654 - val_acc: 0.6553\n",
      "Epoch 15/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.4056 - acc: 0.8173 - val_loss: 0.7121 - val_acc: 0.6574\n",
      "Epoch 16/50\n",
      "1407/1407 [==============================] - 142s - loss: 0.3841 - acc: 0.8323 - val_loss: 0.7091 - val_acc: 0.6617\n",
      "Epoch 17/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.3639 - acc: 0.8408 - val_loss: 0.6882 - val_acc: 0.6745\n",
      "Epoch 18/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.3432 - acc: 0.8579 - val_loss: 0.7108 - val_acc: 0.6702\n",
      "Epoch 19/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.3579 - acc: 0.8408 - val_loss: 0.6783 - val_acc: 0.6553\n",
      "Epoch 20/50\n",
      "1407/1407 [==============================] - 142s - loss: 0.3340 - acc: 0.8650 - val_loss: 0.6810 - val_acc: 0.6723\n",
      "Epoch 21/50\n",
      "1407/1407 [==============================] - 143s - loss: 0.3438 - acc: 0.8593 - val_loss: 0.7065 - val_acc: 0.6702\n",
      "Epoch 22/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.3100 - acc: 0.8756 - val_loss: 0.7211 - val_acc: 0.6809\n",
      "Epoch 23/50\n",
      "1407/1407 [==============================] - 138s - loss: 0.3079 - acc: 0.8706 - val_loss: 0.6533 - val_acc: 0.7021\n",
      "Epoch 24/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.3026 - acc: 0.8706 - val_loss: 0.6544 - val_acc: 0.6957\n",
      "Epoch 25/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2782 - acc: 0.8806 - val_loss: 0.6566 - val_acc: 0.7064\n",
      "Epoch 26/50\n",
      "1407/1407 [==============================] - 141s - loss: 0.2903 - acc: 0.8792 - val_loss: 0.6818 - val_acc: 0.6723\n",
      "Epoch 27/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.2840 - acc: 0.8778 - val_loss: 0.6405 - val_acc: 0.7064\n",
      "Epoch 28/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.2650 - acc: 0.8870 - val_loss: 0.6694 - val_acc: 0.6915\n",
      "Epoch 29/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2630 - acc: 0.8849 - val_loss: 0.6635 - val_acc: 0.6830\n",
      "Epoch 30/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.2689 - acc: 0.8785 - val_loss: 0.6990 - val_acc: 0.6936\n",
      "Epoch 31/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2532 - acc: 0.8856 - val_loss: 0.6953 - val_acc: 0.7106\n",
      "Epoch 32/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.2538 - acc: 0.8820 - val_loss: 0.6812 - val_acc: 0.6894\n",
      "Epoch 33/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.2554 - acc: 0.8941 - val_loss: 0.7191 - val_acc: 0.7021\n",
      "Epoch 34/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2410 - acc: 0.8920 - val_loss: 0.7152 - val_acc: 0.6830\n",
      "Epoch 35/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2614 - acc: 0.8799 - val_loss: 0.7195 - val_acc: 0.7128\n",
      "Epoch 36/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2438 - acc: 0.8806 - val_loss: 0.7084 - val_acc: 0.6915\n",
      "Epoch 37/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2466 - acc: 0.8849 - val_loss: 0.6814 - val_acc: 0.6957\n",
      "Epoch 38/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2320 - acc: 0.8927 - val_loss: 0.6889 - val_acc: 0.6894\n",
      "Epoch 39/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.2433 - acc: 0.8834 - val_loss: 0.6726 - val_acc: 0.6957\n",
      "Epoch 40/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2330 - acc: 0.8905 - val_loss: 0.6648 - val_acc: 0.6915\n",
      "Epoch 41/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2325 - acc: 0.8856 - val_loss: 0.6998 - val_acc: 0.7000\n",
      "Epoch 42/50\n",
      "1407/1407 [==============================] - 138s - loss: 0.2272 - acc: 0.8920 - val_loss: 0.7251 - val_acc: 0.6936\n",
      "Epoch 43/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2329 - acc: 0.8849 - val_loss: 0.6630 - val_acc: 0.6957\n",
      "Epoch 44/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2265 - acc: 0.8891 - val_loss: 0.7100 - val_acc: 0.6915\n",
      "Epoch 45/50\n",
      "1407/1407 [==============================] - 140s - loss: 0.2231 - acc: 0.9005 - val_loss: 0.7338 - val_acc: 0.7106\n",
      "Epoch 46/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2187 - acc: 0.8934 - val_loss: 0.7310 - val_acc: 0.7000\n",
      "Epoch 47/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2237 - acc: 0.8898 - val_loss: 0.7014 - val_acc: 0.7000\n",
      "Epoch 48/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2124 - acc: 0.8955 - val_loss: 0.7929 - val_acc: 0.7021\n",
      "Epoch 49/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.2220 - acc: 0.9005 - val_loss: 0.6774 - val_acc: 0.7191\n",
      "Epoch 50/50\n",
      "1407/1407 [==============================] - 139s - loss: 0.1989 - acc: 0.9048 - val_loss: 0.8214 - val_acc: 0.7043\n",
      "Accuracy: 70.43%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Try running from json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 70.43%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 8s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = loaded_model.predict_classes(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Neural Networking increased the accuracy of my model of baseline: 50% up to 70%. If I wanted to increase the score, I think the following factors could help increase the accuracy.\n",
    "- Image transformations\n",
    "- More epochs\n",
    "- Alter parameters/layers used\n",
    "- More data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
